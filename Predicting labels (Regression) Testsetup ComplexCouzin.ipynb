{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalls = np.array([1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 25, 30, 35, 40, 50, 60, 80, 100,  125, 150, 200, 300, 500, 750, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_per_dataset = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre train\n",
    "import random\n",
    "import tables \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lcs = []\n",
    "for i in range(0, 40):\n",
    "    lcs.append([])\n",
    "windows_data = 0\n",
    "max_i = 0\n",
    "max_v = 0\n",
    "i = 0\n",
    "labels = []\n",
    "data = []\n",
    " \n",
    "with tables.open_file('pathtofile.h5', mode='r') as hdf5_file:\n",
    "    print(\"shape: \", hdf5_file.root.train.shape)\n",
    "    for window in hdf5_file.root.train:\n",
    "        #print(window.shape)\n",
    "        #print(window[0, 4:15])\n",
    "        label = window[-1, 9].astype(int)\n",
    "        #print(window.shape)\n",
    "        lcs[label].append(window)\n",
    "        #print(label)\n",
    "        labels.append(label)\n",
    "hist = plt.hist(labels, bins= range(0,41))\n",
    "plt.hist(labels, bins= range(0,41))\n",
    "print(\"Number of occ: \", hist[0])\n",
    "ps = np.min(hist[0])/ hist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqltrain = []\n",
    "for data_label_i in lcs:\n",
    "    for j in range(0, 4000):\n",
    "        eqltrain.append(data_label_i[j])\n",
    "print(len(eqltrain))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre train\n",
    "import random\n",
    "import tables \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lcs = []\n",
    "for i in range(0, 40):\n",
    "    lcs.append([])\n",
    "windows_data = 0\n",
    "max_i = 0\n",
    "max_v = 0\n",
    "i = 0\n",
    "labels = []\n",
    "data = []\n",
    " \n",
    "with tables.open_file('pathtofile.h5', mode='r') as hdf5_file:\n",
    "    print(\"shape: \", hdf5_file.root.train.shape)\n",
    "    for window in hdf5_file.root.train:\n",
    "        #print(window.shape)\n",
    "        #print(window[0, 4:15])\n",
    "        label = window[-1, 9].astype(int)\n",
    "        #print(window.shape)\n",
    "        lcs[label].append(window)\n",
    "        #print(label)\n",
    "        labels.append(label)\n",
    "hist = plt.hist(labels, bins= range(0,41))\n",
    "plt.hist(labels, bins= range(0,41))\n",
    "print(\"Number of occ: \", hist[0])\n",
    "ps = np.min(hist[0])/ hist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqltest = []\n",
    "for data_label_i in lcs:\n",
    "    for j in range(0, 500):\n",
    "        eqltest.append(data_label_i[j])\n",
    "print(len(eqltest))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_bins = 16\n",
    "window_size = 40\n",
    "img_dtype = tables.Float64Atom()  # dtype in which the images will be saved\n",
    "data_shape = (0, window_size -1, 5 + 6 + 4 + (vision_bins * 8))\n",
    "with tables.open_file('pathtofile.h5', mode='w') as hdf5_file:\n",
    "    hdf5_file.create_earray(hdf5_file.root, 'train', img_dtype, shape = data_shape)\n",
    "    print(hdf5_file.root.train.shape)\n",
    "    for d in eqltest:\n",
    "        hdf5_file.root.train.append(d[None, :, :])\n",
    "    hdf5_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_data(data, batchsize, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, data.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield data[excerpt, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy, quelle noch angeben:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def log_sum_exp(x, dim=1):\n",
    "    x_max, x_argmax = x.max(dim, keepdim=True)\n",
    "    x_max_broadcast = x_max.expand(*x.size())\n",
    "    return x_max + torch.log(\n",
    "        torch.sum(torch.exp(x - x_max_broadcast), dim=dim, keepdim=True))\n",
    "\n",
    "class MDN(nn.Module):\n",
    "    \"\"\"A mixture density network layer\n",
    "    The input maps to the parameters of a MoG probability distribution, where\n",
    "    each Gaussian has O dimensions and diagonal covariance.\n",
    "    Arguments:\n",
    "        in_features (int): the number of dimensions in the input\n",
    "        out_features (int): the number of dimensions in the output\n",
    "        num_gaussians (int): the number of Gaussians per output dimensions\n",
    "    Input:\n",
    "        minibatch (BxD): B is the batch size and D is the number of input\n",
    "            dimensions.\n",
    "    Output:\n",
    "        (pi, sigma, mu) (BxG, BxGxO, BxGxO): B is the batch size, G is the\n",
    "            number of Gaussians, and O is the number of dimensions for each\n",
    "            Gaussian. Pi is a multinomial distribution of the Gaussians. Sigma\n",
    "            is the standard deviation of each Gaussian. Mu is the mean of each\n",
    "            Gaussian.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.pi = nn.Sequential(\n",
    "            nn.Linear(in_features, num_gaussians), nn.LogSoftmax(dim=1))\n",
    "        self.sigma = nn.Linear(in_features, out_features * num_gaussians)\n",
    "        self.mu = nn.Linear(in_features, out_features * num_gaussians)\n",
    "\n",
    "    def forward(self, minibatch):\n",
    "        pi = self.pi(minibatch)\n",
    "        sigma = self.sigma(minibatch)\n",
    "        # original sigma = torch.clamp(sigma, np.log(np.sqrt(1e-4)), 1e8)\n",
    "        # working \n",
    "        sigma =  torch.clamp(sigma, np.log(np.sqrt(1e-3)), 5e1)\n",
    "        #try 3 sigma = torch.clamp(sigma, np.log(np.sqrt(1e-3)), 1e4)\n",
    "        sigma = sigma.view(-1, self.num_gaussians, self.out_features)\n",
    "        mu = self.mu(minibatch)\n",
    "        mu = mu.view(-1, self.num_gaussians, self.out_features)\n",
    "        return pi, sigma, mu\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_probability(sigma, x_mu, x):\n",
    "        \"\"\"Returns the probability of `data` given MoG parameters `sigma` and `mu`.\n",
    "        Arguments:\n",
    "            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n",
    "                size, G is the number of Gaussians, and O is the number of\n",
    "                dimensions per Gaussian.\n",
    "            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n",
    "                number of Gaussians, and O is the number of dimensions per Gaussian.\n",
    "            data (BxI): A batch of data. B is the batch size and I is the number of\n",
    "                input dimensions.\n",
    "        Returns:\n",
    "            probabilities (BxG): The probability of each point in the probability\n",
    "                of the distribution in the corresponding sigma/mu index.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1).expand_as(sigma)\n",
    "        var = (torch.exp(sigma)**2)\n",
    "        return -((x - x_mu)**2) / (2 * var + 1e-4) - sigma - math.log(\n",
    "            math.sqrt(2 * math.pi))\n",
    "\n",
    "    @staticmethod\n",
    "    def mdn_loss(pi, sigma, mu, target):\n",
    "        \"\"\"Calculates the error, given the MoG parameters and the target\n",
    "        The loss is the negative log likelihood of the data given the MoG\n",
    "        parameters.\n",
    "        \"\"\"\n",
    "        nll = log_sum_exp(pi[:, :, None] +\n",
    "                          MDN.gaussian_probability(sigma, mu, target))\n",
    "        nll = -torch.sum(nll, dim=-1)\n",
    "        return torch.mean(nll)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(pi, sigma, mu):\n",
    "        \"\"\"Draw samples from a MoG.\n",
    "        \"\"\"\n",
    "        categorical = Categorical(torch.exp(pi))\n",
    "        pis = list(categorical.sample().data)\n",
    "        sigma = torch.exp(sigma)\n",
    "        sample = Variable(\n",
    "            sigma.data.new(sigma.size(0), sigma.size(2)).normal_())\n",
    "        for i, idx in enumerate(pis):\n",
    "            sample[i] = sample[i].mul(sigma[i, idx]).add(mu[i, idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    if type(model) in [nn.Linear]:\n",
    "        nn.init.xavier_normal_(model.weight.data)\n",
    "    elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "        nn.init.xavier_normal_(model.weight_hh_l0)\n",
    "        nn.init.xavier_normal_(model.weight_ih_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_outputs):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # 32 was used for all the simulated data\n",
    "        #hidden_dim = 32 #\n",
    "        \n",
    "        hidden_dim = 128 #hidden_dim\n",
    "\n",
    "        #self.inp = torch.nn.Linear(n_features, hidden_size)\n",
    "        num_layers = 2\n",
    "        #self.rnn = LayerNormLSTM(n_features, hidden_dim, num_layers = num_layers)\n",
    "        self.rnn = torch.nn.LSTM(n_features, hidden_dim, num_layers = num_layers)\n",
    "        \n",
    "        # 64 was used for all the simulated data\n",
    "        #self.out = torch.nn.Linear(hidden_dim, 64)\n",
    "        \n",
    "        #self.out = torch.nn.Linear(hidden_dim, 32)\n",
    "        self.mdn = MDN(hidden_dim, n_outputs, 5)\n",
    "\n",
    "        \n",
    "        #self.hidden = None\n",
    "        \n",
    "        initialize_weights(self.rnn)\n",
    "        #initialize_weights(self.out)\n",
    "        initialize_weights(self.mdn)\n",
    "\n",
    "    def step(self, inputs, hidden=None, verbose=False):\n",
    "        #input = self.inp(input)\n",
    "        if verbose:\n",
    "            print(\"Step 0:\")\n",
    "            print(inputs.shape)\n",
    "        inputs = inputs.permute([1, 0, 2])\n",
    "        if verbose:\n",
    "            print(\"Step 1:\")\n",
    "            print(inputs.shape)\n",
    "        #self.rnn.flatten_parameters()\n",
    "        output, hidden = self.rnn(inputs, hidden)\n",
    "        output = output[-1, :, :] #output[:, :, :] #output[-1, :, :]\n",
    "        #output = output.permute([1, 0, 2])\n",
    "        if verbose:\n",
    "            print(\"Step 3:\")\n",
    "            print(output.shape)\n",
    "        output = output.squeeze()\n",
    "        if verbose:\n",
    "            print(\"Step 4:\")\n",
    "            print(output.shape)\n",
    "        #output = self.out(output)\n",
    "        if verbose:\n",
    "            print(\"Step 5:\")\n",
    "            print(output.shape)\n",
    "            print(output)\n",
    "        output = self.mdn(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, inputs, hidden=None, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"inputs size: \", inputs.size)\n",
    "        batch_size = inputs.size(0)    \n",
    "        output, hidden = self.step(inputs, hidden, verbose=verbose)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import tables \n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class HiddenClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        super(HiddenClassifier, self).__init__() \n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, output_dim),\n",
    "            #torch.nn.Softmax()#torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class HiddenClassifierRNN(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_outputs):\n",
    "        super(HiddenClassifierRNN, self).__init__()\n",
    "        \n",
    "        hidden_dim = 128 #hidden_dim\n",
    "\n",
    "        num_layers = 1\n",
    "        self.rnn = torch.nn.LSTM(n_features, hidden_dim, num_layers = num_layers)\n",
    "        #self.out = torch.nn.Linear(hidden_dim, n_outputs)\n",
    "        #self.softmax = nn.Sigmoid()\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, n_outputs),\n",
    "            #torch.nn.Softmax()\n",
    "        )\n",
    "                    \n",
    "\n",
    "        \n",
    "        #self.hidden = None\n",
    "        \n",
    "        initialize_weights(self.rnn)\n",
    "        initialize_weights(self.out)\n",
    "\n",
    "    def step(self, inputs, hidden=None, verbose=False):\n",
    "        #input = self.inp(input)\n",
    "        inputs = inputs.permute([1, 0, 2])\n",
    "        output, hidden = self.rnn(inputs, hidden)\n",
    "        output = output[-1, :, :] #output[:, :, :] #output[-1, :, :]\n",
    "        output = output.squeeze()\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, inputs, hidden=None, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"inputs\", inputs.shape)\n",
    "            print(\"inputs size: \", inputs.size)\n",
    "        batch_size = inputs.size(0)    \n",
    "        output, hidden = self.step(inputs, hidden, verbose=verbose)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hmodel = HiddenClassifierRNN(256, 1).cuda()\n",
    "#hmodel = HiddenClassifier(256, 40).cuda()\n",
    "#äweight = torch.tensor([0.00000001, 0.1 , 1 , 1]).cuda()\n",
    "#criterion = nn.CrossEntropyLoss()# Mean Squared Loss\n",
    "criterion = nn.MSELoss()# Mean Squared Loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "l_rate = 0.001 \n",
    "#optimizer = torch.optim.SGD(hmodel.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(hmodel.parameters(), lr=0.001)\n",
    "hmodel\n",
    "\n",
    "#constrnn = SimpleRNN(n_features=5 + (vision_bins * 8), n_outputs=4).cuda()\n",
    "#sclass_model = SimpleClassifier(133, 1).cuda()\n",
    "#constrnn = torch.load('asdf.pt')\n",
    "#constrnn = torch.load('constcomplex1rnntrain.pt')\n",
    "\n",
    "#weight = torch.tensor([0.88675, 0.2187 , 3 , 3]).cuda()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "#optimizer = torch.optim.SGD(sclass_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#criterion = nn.MSELoss()# Mean Squared Loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#l_rate = 0.001 \n",
    "#optimizer = torch.optim.SGD(hmodel.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(sclass_model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(sclass_model.parameters(), lr=0.005)\n",
    "#optimizer = torch.optim.RMSprop(sclass_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmodel = HiddenClassifier(256, 1).cuda()\n",
    "#hmodel = HiddenClassifier(256, 40).cuda()\n",
    "#äweight = torch.tensor([0.00000001, 0.1 , 1 , 1]).cuda()\n",
    "#criterion = nn.CrossEntropyLoss()# Mean Squared Loss\n",
    "criterion = nn.MSELoss()# Mean Squared Loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "l_rate = 0.001 \n",
    "#optimizer = torch.optim.SGD(hmodel.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(hmodel.parameters(), lr=0.001)\n",
    "hmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_bins = 16\n",
    "#rnn = SimpleRNN(n_features=5 + (vision_bins * 8), n_outputs=4).cuda()\n",
    "rnn_model = SimpleRNN(n_features=5 +(vision_bins * 8), n_outputs=4).cuda()\n",
    "#rnn = torch.load('asd.pt')\n",
    "#rnn_model = torch.load('memsteprnn.pt')\n",
    "rnn_model = torch.load('complexnew-rnn.pt')\n",
    "rnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(labels, C):\n",
    "    A = np.zeros((labels.shape[0], C))\n",
    "    for i in range(labels.shape[0]):\n",
    "        A[i, min(C-1, labels[i].astype('int'))] = 1\n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_wall_labels(labels):\n",
    "    for i in range(labels.shape[0]):\n",
    "        if labels[i] > 2:\n",
    "            labels[i] = 3\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_per_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "import time\n",
    "import tables\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "data_from_runs = []\n",
    "#vision_bins = 16\n",
    "#rnn = SimpleRNN(n_features=5 + (vision_bins * 8), n_outputs=4).cuda()\n",
    "#rnn_model = SimpleRNN(n_features=5 +(vision_bins * 8), n_outputs=4).cuda()\n",
    "#rnn = torch.load('asd.pt')\n",
    "#rnn_model = torch.load('memsteprnn.pt')\n",
    "#rnn_model.eval()\n",
    "eqltrain = np.asarray(eqltrain)\n",
    "h5ftest = tables.open_file('pathtofile.h5', mode='r')\n",
    "test = h5ftest.root.train[:, :, :]\n",
    "print(\"lala test shape: \", test.shape)\n",
    "data = []\n",
    "\n",
    "for samples_per_class in intervalls:\n",
    "    intervall_runs = []\n",
    "    for runs_per_intervall in range(0,runs_per_dataset):\n",
    "        print(runs_per_intervall + 1, \". run with \", samples_per_class, \"samples per class\")\n",
    "        batch_size = 32\n",
    "        train = []\n",
    "        print(\"eqltrain shape\", eqltrain.shape[0])\n",
    "        train = eqltrain[np.random.choice(eqltrain.shape[0], samples_per_class*40), :, :]\n",
    "        #train = eqltrain[:40000, :, :]\n",
    "        hmodel = HiddenClassifier(256, 1).cuda()\n",
    "        criterion = nn.MSELoss()# Mean Squared Loss\n",
    "        l_rate = 0.001 \n",
    "        optimizer = torch.optim.Adam(hmodel.parameters(), lr=0.001)\n",
    "        train = np.asarray(train)\n",
    "        np.random.shuffle(train) #inplace op\n",
    "        print(train.shape)\n",
    "        print(\"samples per classes: \", train.shape[0]/40)\n",
    "        rec_losses = []\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "    \n",
    "        epochs = int(max(np.round(20000 / train.shape[0]), 10))\n",
    "        #TODO: MORE EPOCHS\n",
    "        #epochs = int(max(np.round(20000 / train.shape[0]), 10))\n",
    "        print(\"Epochs: \", epochs)\n",
    "        tqdm_epochs = tqdm_notebook(list(range(epochs)))\n",
    "        labels = []\n",
    "\n",
    "        tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "                                   total= 1)#train.shape[0] // batch_size, leave = False)\n",
    "        for epoch in range(0, epochs): #tqdm_epochs: \n",
    "            np.random.shuffle(train)\n",
    "            #tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "            #                           total= train.shape[0] // batch_size, leave = False)#train.shape[0] // batch_size, leave = False)\n",
    "            for minibatch_idx, (batch_data) in enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)):\n",
    "            #for minibatch_idx, (batch_data) in tqdm_iter: \n",
    "                i = i+1\n",
    "                batch_X = batch_data[:, :-1, 6+4:].astype(np.float32) \n",
    "                labels = batch_data[:, -1, 9].astype(np.float32) \n",
    "                #print(labels)\n",
    "                #print(batch_data[:, -1, 6:12].astype(np.float32) )\n",
    "                batch_Y = labels\n",
    "                with autograd.detect_anomaly():\n",
    "                    hmodel.zero_grad()\n",
    "                    batch_X_rnn = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "                    batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "                    t, hidden_input = rnn_model.forward(batch_X_rnn, verbose = False)\n",
    "                    #print(hidden_input[0].shape)\n",
    "                    #print(hidden_input[1].shape)\n",
    "                    batch_X = torch.cat((hidden_input[0][1], hidden_input[1][1]), 1)\n",
    "                    batch_X = torch.autograd.Variable(batch_X).cuda()\n",
    "                    outputs = hmodel.forward(batch_X) #847\n",
    "                    loss = criterion(outputs.view(-1), batch_Y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "                    y_preds = np.concatenate((y_preds, y_pred))\n",
    "                    y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "                    y_trues = np.concatenate((y_trues, y_true))\n",
    "                    rec_losses.append(float(loss.cpu().data.numpy()))\n",
    "                    tqdm_epochs.set_description(\"Loss {:6.5f}\".format(np.mean(rec_losses[-100:])))   \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(16,4))\n",
    "        ax.plot(pd.Series(rec_losses).rolling(100).mean())\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "        batch_size = 2\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "        label_counts = []\n",
    "\n",
    "        for batch_data in iterate_data(test, batchsize=batch_size, shuffle=False):\n",
    "            batch_X = batch_data[:, :-1, 6+4:].astype(np.float32)\n",
    "            labels = batch_data[:, -1, 9].astype(np.float32)\n",
    "            batch_Y = labels #group_wall_labels(labels)\n",
    "            batch_X_rnn = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "            batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "            t, hidden_input = rnn_model.forward(batch_X_rnn, verbose = False)\n",
    "            batch_X = torch.cat((hidden_input[0][1], hidden_input[1][1]), 1)\n",
    "            batch_X = torch.autograd.Variable(batch_X).cuda()\n",
    "            outputs = hmodel.forward(batch_X) #847\n",
    "            loss = criterion(outputs.view(-1), batch_Y)\n",
    "            y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "            y_preds = np.concatenate((y_preds, y_pred))\n",
    "            y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "            y_trues = np.concatenate((y_trues, y_true))\n",
    "        y_trues_oh = make_one_hot(np.round(y_trues), 40)\n",
    "        y_preds_oh = make_one_hot(np.round(y_preds), 40)\n",
    "    \n",
    "        # Split the data into a training set and a test set\n",
    "        y_test = np.argmax(y_trues_oh, axis = 1)\n",
    "        y_pred = np.argmax(np.round(y_preds_oh), axis = 1)\n",
    "        np.set_printoptions(precision=2)\n",
    "        # Plot non-normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None)\n",
    "        # Plot normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None, normalize=True)\n",
    "        #plt.show()\n",
    "    \n",
    "        errs = np.abs(y_test-y_pred)\n",
    "        plt.hist(errs)\n",
    "        plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        print(\"error not bigger than 5: \", np.sum(errs < 5)/errs.shape[0])\n",
    "    \n",
    "        mean_err = np.sum(errs) / y_test.shape[0]\n",
    "        print(\"Durchschnittliche Abweichung: \", mean_err)\n",
    "    \n",
    "        standard_derivation_err = np.sqrt(1/(y_test.shape[0]-1) * np.sum(np.square(mean_err-errs)))\n",
    "        print(\"standard abweichung: \", standard_derivation_err)\n",
    "\n",
    "        cm = metrics.confusion_matrix(np.argmax(y_trues_oh, axis = 1), np.argmax(np.round(y_preds_oh), axis = 1), labels=None, sample_weight=None)\n",
    "        #print(cm.shape)    \n",
    "        print(\"richtig klassifiziert: \", cm.trace()/y_test.shape[0])\n",
    "        #data.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    \n",
    "        plt.imshow(cm)\n",
    "        plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        plt.show()\n",
    "        \n",
    "        intervall_runs.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    data.append(np.asarray(intervall_runs))\n",
    "    print(\"intervall_runs shape: \", len(intervall_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datameans = np.mean(np.asarray(data), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datameans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_errorsr = []\n",
    "std_correctsr = []\n",
    "for intervall_data in data:\n",
    "    print(intervall_data)\n",
    "    leninterveall = intervall_data.shape[0]\n",
    "    mean_error = np.mean(intervall_data[:, 1])\n",
    "    mean_correct = np.mean(intervall_data[:, 3])\n",
    "    std_errorsr.append(np.sqrt(1/leninterveall * \n",
    "                            np.sum((intervall_data[:, 1] - mean_error)\n",
    "                                   *(intervall_data[:, 1] - mean_error))))\n",
    "    std_correctsr.append(np.sqrt(1/leninterveall * \n",
    "                        np.sum((intervall_data[:, 3] - mean_correct)\n",
    "                                *(intervall_data[:, 3] - mean_correct))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanp = datameans\n",
    "datanp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datanp[:, 0], datanp[:, 1], 'ro')\n",
    "plt.ylabel('some numbers')\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = datanp[:, 0]\n",
    "y = datanp[:, 1]\n",
    "e = datanp[:, 2]\n",
    "\n",
    "plt.errorbar(x, y, e, linestyle='None', marker='^')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datanp[:, 0], datanp[:, 3]*100, 'ro-')\n",
    "plt.ylabel('some numbers')\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class SimpleClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_outputs):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        hidden_dim = 128 #hidden_dim\n",
    "\n",
    "        num_layers = 1\n",
    "        self.rnn = torch.nn.LSTM(n_features, hidden_dim, num_layers = num_layers)\n",
    "        #self.out = torch.nn.Linear(hidden_dim, n_outputs)\n",
    "        #self.softmax = nn.Sigmoid()\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, n_outputs),\n",
    "            #torch.nn.Softmax()\n",
    "        )\n",
    "                    \n",
    "\n",
    "        \n",
    "        #self.hidden = None\n",
    "        \n",
    "        initialize_weights(self.rnn)\n",
    "        initialize_weights(self.out)\n",
    "\n",
    "    def step(self, inputs, hidden=None, verbose=False):\n",
    "        #input = self.inp(input)\n",
    "        inputs = inputs.permute([1, 0, 2])\n",
    "        output, hidden = self.rnn(inputs, hidden)\n",
    "        output = output[-1, :, :] #output[:, :, :] #output[-1, :, :]\n",
    "        output = output.squeeze()\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, inputs, hidden=None, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"inputs\", inputs.shape)\n",
    "            print(\"inputs size: \", inputs.size)\n",
    "        batch_size = inputs.size(0)    \n",
    "        output, hidden = self.step(inputs, hidden, verbose=verbose)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constrnn = SimpleRNN(n_features=5 + (vision_bins * 8), n_outputs=4).cuda()\n",
    "sclass_model = SimpleClassifier(133, 1).cuda()\n",
    "#constrnn = torch.load('asdf.pt')\n",
    "#constrnn = torch.load('constcomplex1rnntrain.pt')\n",
    "\n",
    "#weight = torch.tensor([0.88675, 0.2187 , 3 , 3]).cuda()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "#optimizer = torch.optim.SGD(sclass_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = nn.MSELoss()# Mean Squared Loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "l_rate = 0.001 \n",
    "#optimizer = torch.optim.SGD(hmodel.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(sclass_model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(sclass_model.parameters(), lr=0.005)\n",
    "#optimizer = torch.optim.RMSprop(sclass_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "import time\n",
    "import tables\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "data_from_runs = []\n",
    "\n",
    "\n",
    "eqltrain = np.asarray(eqltrain)\n",
    "h5ftest = tables.open_file('pathtofile.h5', mode='r')\n",
    "test = h5ftest.root.train[:, :, :]\n",
    "#print(\"lala test shape: \", test.shape)\n",
    "datarawrnn = []\n",
    "\n",
    "for samples_per_class in intervalls:\n",
    "    intervall_runs = []\n",
    "    for runs_per_intervall in range(0,runs_per_dataset):\n",
    "        print(runs_per_intervall + 1, \". run with \", samples_per_class, \"samples per class\")\n",
    "        batch_size = 32\n",
    "        train = []\n",
    "        train = eqltrain[np.random.choice(eqltrain.shape[0], samples_per_class*40), :, :]\n",
    "        sclass_model = SimpleClassifier(133, 1).cuda()\n",
    "        criterion = nn.MSELoss()# Mean Squared Loss\n",
    "        l_rate = 0.001 \n",
    "        optimizer = torch.optim.Adam(sclass_model.parameters(), lr=0.001)\n",
    "        train = np.array(train)\n",
    "        np.random.shuffle(train) #inplace op\n",
    "        print(train.shape)\n",
    "        print(\"samples per classes: \", train.shape[0]/40)\n",
    "        rec_losses = []\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "    \n",
    "        epochs = int(max(np.round(50000 / train.shape[0]), 10))\n",
    "        #TODO: MORE EPOCHS\n",
    "        #epochs = int(max(np.round(20000 / train.shape[0]), 10))\n",
    "        print(\"Epochs: \", epochs)\n",
    "        tqdm_epochs = tqdm_notebook(list(range(epochs)))\n",
    "        labels = []\n",
    "\n",
    "        tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "                                   total= 1)#train.shape[0] // batch_size, leave = False)\n",
    "        for epoch in range(0, epochs): #tqdm_epochs: \n",
    "            np.random.shuffle(train)\n",
    "            #tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "            #                           total= train.shape[0] // batch_size, leave = False)#train.shape[0] // batch_size, leave = False)\n",
    "            for minibatch_idx, (batch_data) in enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)):\n",
    "            #for minibatch_idx, (batch_data) in tqdm_iter: \n",
    "                i = i+1\n",
    "                batch_X = batch_data[:, :-1, 6+4:].astype(np.float32) \n",
    "                #print(\"bx shape\", batch_X.shape)\n",
    "                labels = batch_data[:, -1, 9].astype(np.float32) \n",
    "                #print(labels)\n",
    "                #print(batch_data[:, -1, 6:12].astype(np.float32) )\n",
    "                batch_Y = labels\n",
    "                #print(\"by shape\", batch_Y.shape)\n",
    "                with autograd.detect_anomaly():\n",
    "                    sclass_model.zero_grad()\n",
    "                    batch_X = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "                    batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "                    outputs, hidden_input = sclass_model.forward(batch_X, verbose = False)\n",
    "                    loss = criterion(outputs.view(-1), batch_Y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "                    y_preds = np.concatenate((y_preds, y_pred))\n",
    "                    y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "                    y_trues = np.concatenate((y_trues, y_true))\n",
    "                    rec_losses.append(float(loss.cpu().data.numpy()))\n",
    "                    tqdm_epochs.set_description(\"Loss {:6.5f}\".format(np.mean(rec_losses[-100:])))   \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(16,4))\n",
    "        ax.plot(pd.Series(rec_losses).rolling(100).mean())\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "        batch_size = 2\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "        label_counts = []\n",
    "\n",
    "        for batch_data in iterate_data(test, batchsize=batch_size, shuffle=False):\n",
    "            batch_X = batch_data[:, :-1, 6+4:].astype(np.float32)\n",
    "            labels = batch_data[:, -1, 9].astype(np.float32)\n",
    "            batch_Y = labels #group_wall_labels(labels)\n",
    "            batch_X = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "            batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "            outputs, hidden_input = sclass_model.forward(batch_X, verbose = False)\n",
    "            loss = criterion(outputs.view(-1), batch_Y)\n",
    "            y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "            y_preds = np.concatenate((y_preds, y_pred))\n",
    "            y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "            y_trues = np.concatenate((y_trues, y_true))\n",
    "        y_trues_oh = make_one_hot(np.round(y_trues), 40)\n",
    "        y_preds_oh = make_one_hot(np.round(y_preds), 40)\n",
    "    \n",
    "        # Split the data into a training set and a test set\n",
    "        y_test = np.argmax(y_trues_oh, axis = 1)\n",
    "        y_pred = np.argmax(np.round(y_preds_oh), axis = 1)\n",
    "        np.set_printoptions(precision=2)\n",
    "        # Plot non-normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None)\n",
    "        # Plot normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None, normalize=True)\n",
    "        #plt.show()\n",
    "    \n",
    "        errs = np.abs(y_test-y_pred)\n",
    "        plt.hist(errs)\n",
    "        plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        print(\"error not bigger than 5: \", np.sum(errs < 5)/errs.shape[0])\n",
    "    \n",
    "        mean_err = np.sum(errs) / y_test.shape[0]\n",
    "        print(\"Durchschnittliche Abweichung: \", mean_err)\n",
    "    \n",
    "        standard_derivation_err = np.sqrt(1/(y_test.shape[0]-1) * np.sum(np.square(mean_err-errs)))\n",
    "        print(\"standard abweichung: \", standard_derivation_err)\n",
    "\n",
    "        cm = metrics.confusion_matrix(np.argmax(y_trues_oh, axis = 1), np.argmax(np.round(y_preds_oh), axis = 1), labels=None, sample_weight=None)\n",
    "        #print(cm.shape)    \n",
    "        print(\"richtig klassifiziert: \", cm.trace()/y_test.shape[0])\n",
    "        #data.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    \n",
    "        #plt.imshow(cm)\n",
    "        #plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        #plt.show()\n",
    "        \n",
    "        intervall_runs.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    datarawrnn.append(np.asarray(intervall_runs))\n",
    "    print(\"intervall_runs shape: \", len(intervall_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_errorrr = []\n",
    "std_correctrr = []\n",
    "for intervall_data in datarawrnn:\n",
    "    print(intervall_data)\n",
    "    leninterveall = intervall_data.shape[0]\n",
    "    mean_error = np.mean(intervall_data[:, 1])\n",
    "    mean_correct = np.mean(intervall_data[:, 3])\n",
    "    std_errorrr.append(np.sqrt(1/leninterveall * \n",
    "                            np.sum((intervall_data[:, 1] - mean_error)\n",
    "                                   *(intervall_data[:, 1] - mean_error))))\n",
    "    std_correctrr.append(np.sqrt(1/leninterveall * \n",
    "                        np.sum((intervall_data[:, 3] - mean_correct)\n",
    "                                *(intervall_data[:, 3] - mean_correct))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanprawrnn = np.mean(np.asarray(datarawrnn), axis = 1)\n",
    "datanprawrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.plot(datanp[:, 0], datanp[:, 1], 'ro', label=\"Hidden + RNN)\")\n",
    "rawrnnline = plt.plot(datanprawrnn[:, 0], datanprawrnn[:, 1], 'bo', label=\"Raw Data + RNN)\")\n",
    "plt.ylabel('Average error')\n",
    "plt.xscale('linear')\n",
    "\n",
    "plt.legend() #(loc=2, fontsize=\"small\")\n",
    "#plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.errorbar(datanp[:, 0], datanp[:, 1], datanp[:, 2], linestyle='None', marker='^')\n",
    "rawrnnline = plt.errorbar(datanprawrnn[:, 0], datanprawrnn[:, 1], datanprawrnn[:, 2], linestyle='None', marker='^')\n",
    "\n",
    "plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datanprawrnn[:, 0], datanprawrnn[:, 3]*100, 'ro-')\n",
    "plt.ylabel('some numbers')\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datanp[:, 0], datanp[:, 3]*100, 'ro-')\n",
    "plt.plot(datanprawrnn[:, 0], datanprawrnn[:, 3]*100, 'bo-')\n",
    "plt.ylabel('Richtig klassifiziert')\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class SC(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        super(SC, self).__init__() \n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose = False):\n",
    "        # Here the forward pass is simply a linear functio\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constrnn = SimpleRNN(n_features=5 + (vision_bins * 8), n_outputs=4).cuda()\n",
    "sc_model = SC(33*133, 1).cuda()\n",
    "#constrnn = torch.load('asdf.pt')\n",
    "#constrnn = torch.load('constcomplex1rnntrain.pt')\n",
    "\n",
    "#weight = torch.tensor([0.88675, 0.2187 , 3 , 3]).cuda()\n",
    "criterion = torch.nn.MSELoss()\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(sc_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "import time\n",
    "import tables\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "data_from_runs = []\n",
    "\n",
    "\n",
    "eqltrain = np.asarray(eqltrain)\n",
    "h5ftest = tables.open_file('pathtofile.h5', mode='r')\n",
    "test = h5ftest.root.train[:, :, :]\n",
    "#print(\"lala test shape: \", test.shape)\n",
    "datarawsimple = []\n",
    "\n",
    "for samples_per_class in intervalls:\n",
    "    intervall_runs = []\n",
    "    for runs_per_intervall in range(0,runs_per_dataset):\n",
    "        print(runs_per_intervall + 1, \". run with \", samples_per_class, \"samples per class\")\n",
    "        batch_size = 32\n",
    "        train = []\n",
    "        train = eqltrain[np.random.choice(eqltrain.shape[0], samples_per_class*40), :, :]\n",
    "        sc_model = SC(38*133, 1).cuda()\n",
    "        criterion = nn.MSELoss()# Mean Squared Loss\n",
    "        l_rate = 0.001 \n",
    "        optimizer = torch.optim.Adam(sc_model.parameters(), lr=0.001)\n",
    "        train = np.array(train)\n",
    "        np.random.shuffle(train) #inplace op\n",
    "        print(train.shape)\n",
    "        print(\"samples per classes: \", train.shape[0]/40)\n",
    "        rec_losses = []\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "    \n",
    "        epochs = int(max(np.round(50000 / train.shape[0]), 10))\n",
    "        #TODO: MORE EPOCHS\n",
    "        #epochs = int(max(np.round(20000 / train.shape[0]), 10))\n",
    "        print(\"Epochs: \", epochs)\n",
    "        tqdm_epochs = tqdm_notebook(list(range(epochs)))\n",
    "        labels = []\n",
    "\n",
    "        tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "                                   total= 1)#train.shape[0] // batch_size, leave = False)\n",
    "        for epoch in range(0, epochs): #tqdm_epochs: \n",
    "            np.random.shuffle(train)\n",
    "            #tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "            #                           total= train.shape[0] // batch_size, leave = False)#train.shape[0] // batch_size, leave = False)\n",
    "            for minibatch_idx, (batch_data) in enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)):\n",
    "            #for minibatch_idx, (batch_data) in tqdm_iter: \n",
    "                i = i+1\n",
    "                batch_X = batch_data[:, :-1, 6+4:].astype(np.float32) \n",
    "                batch_X = batch_X.reshape(batch_size,38*133)\n",
    "                #print(\"bx shape\", batch_X.shape)\n",
    "                labels = batch_data[:, -1, 9].astype(np.float32) \n",
    "                #print(labels)\n",
    "                #print(batch_data[:, -1, 6:12].astype(np.float32) )\n",
    "                batch_Y = labels\n",
    "                #print(\"by shape\", batch_Y.shape)\n",
    "                with autograd.detect_anomaly():\n",
    "                    sc_model.zero_grad()\n",
    "                    batch_X = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "                    batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "                    outputs = sc_model.forward(batch_X, verbose = False)\n",
    "                    loss = criterion(outputs.view(-1), batch_Y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "                    y_preds = np.concatenate((y_preds, y_pred))\n",
    "                    y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "                    y_trues = np.concatenate((y_trues, y_true))\n",
    "                    rec_losses.append(float(loss.cpu().data.numpy()))\n",
    "                    tqdm_epochs.set_description(\"Loss {:6.5f}\".format(np.mean(rec_losses[-100:])))   \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(16,4))\n",
    "        ax.plot(pd.Series(rec_losses).rolling(100).mean())\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "        batch_size = 2\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "        label_counts = []\n",
    "\n",
    "        for batch_data in iterate_data(test, batchsize=batch_size, shuffle=False):\n",
    "            batch_X = batch_data[:, :-1, 6+4:].astype(np.float32)\n",
    "            batch_X = batch_X.reshape(batch_size,38*133)\n",
    "            labels = batch_data[:, -1, 9].astype(np.float32)\n",
    "            batch_Y = labels #group_wall_labels(labels)\n",
    "            batch_X = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "            batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "            outputs = sc_model.forward(batch_X) #847\n",
    "            loss = criterion(outputs.view(-1), batch_Y)\n",
    "            y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "            y_preds = np.concatenate((y_preds, y_pred))\n",
    "            y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "            y_trues = np.concatenate((y_trues, y_true))\n",
    "        y_trues_oh = make_one_hot(np.round(y_trues), 40)\n",
    "        y_preds_oh = make_one_hot(np.round(y_preds), 40)\n",
    "    \n",
    "        # Split the data into a training set and a test set\n",
    "        y_test = np.argmax(y_trues_oh, axis = 1)\n",
    "        y_pred = np.argmax(np.round(y_preds_oh), axis = 1)\n",
    "        np.set_printoptions(precision=2)\n",
    "        # Plot non-normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None)\n",
    "        # Plot normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None, normalize=True)\n",
    "        #plt.show()\n",
    "    \n",
    "        errs = np.abs(y_test-y_pred)\n",
    "        plt.hist(errs)\n",
    "        plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        print(\"error not bigger than 5: \", np.sum(errs < 5)/errs.shape[0])\n",
    "    \n",
    "        mean_err = np.sum(errs) / y_test.shape[0]\n",
    "        print(\"Durchschnittliche Abweichung: \", mean_err)\n",
    "    \n",
    "        standard_derivation_err = np.sqrt(1/(y_test.shape[0]-1) * np.sum(np.square(mean_err-errs)))\n",
    "        print(\"standard abweichung: \", standard_derivation_err)\n",
    "\n",
    "        cm = metrics.confusion_matrix(np.argmax(y_trues_oh, axis = 1), np.argmax(np.round(y_preds_oh), axis = 1), labels=None, sample_weight=None)\n",
    "        #print(cm.shape)    \n",
    "        print(\"richtig klassifiziert: \", cm.trace()/y_test.shape[0])\n",
    "        #data.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    \n",
    "        #plt.imshow(cm)\n",
    "        #plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        #plt.show()\n",
    "        \n",
    "        intervall_runs.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "        #clear_output()\n",
    "    datarawsimple.append(np.asarray(intervall_runs))\n",
    "    print(\"intervall_runs shape: \", len(intervall_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_errorrs = []\n",
    "std_correctrs = []\n",
    "for intervall_data in datarawsimple:\n",
    "    print(intervall_data)\n",
    "    leninterveall = intervall_data.shape[0]\n",
    "    mean_error = np.mean(intervall_data[:, 1])\n",
    "    mean_correct = np.mean(intervall_data[:, 3])\n",
    "    std_errorrs.append(np.sqrt(1/leninterveall * \n",
    "                            np.sum((intervall_data[:, 1] - mean_error)\n",
    "                                   *(intervall_data[:, 1] - mean_error))))\n",
    "    std_correctrs.append(np.sqrt(1/leninterveall * \n",
    "                        np.sum((intervall_data[:, 3] - mean_correct)\n",
    "                                *(intervall_data[:, 3] - mean_correct))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanprawsimple = np.mean(np.asarray(datarawsimple), axis = 1)\n",
    "datanprawrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.plot(datanp[:, 0], datanp[:, 1], 'ro-', label=\"Hidden + RNN)\")\n",
    "rawrnnline = plt.plot(datanprawrnn[:, 0], datanprawrnn[:, 1], 'bo-', label=\"Raw Data + RNN)\")\n",
    "rawsimpleline = plt.plot(datanprawsimple[:, 0], datanprawsimple[:, 1], 'go-', label=\"Raw Data + Simple)\")\n",
    "plt.ylabel('Average error')\n",
    "plt.xscale('linear')\n",
    "\n",
    "plt.legend() #(loc=2, fontsize=\"small\")\n",
    "#plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.errorbar(datanp[:, 0], datanp[:, 1], datanp[:, 2], linestyle='None', marker='^')\n",
    "rawrnnline = plt.errorbar(datanprawrnn[:, 0], datanprawrnn[:, 1], datanprawrnn[:, 2], linestyle='None', marker='^')\n",
    "rawsimpleline = plt.errorbar(datanprawsimple[:, 0], datanprawsimple[:, 1], datanprawsimple[:, 2], linestyle='None', marker='^')\n",
    "\n",
    "plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datanp[:, 0], datanp[:, 3]*100, 'ro-')\n",
    "plt.plot(datanprawrnn[:, 0], datanprawrnn[:, 3]*100, 'bo-')\n",
    "plt.plot(datanprawsimple[:, 0], datanprawsimple[:, 3]*100, 'go-')\n",
    "plt.ylabel('Richtig klassifiziert')\n",
    "plt.xscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple last timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class SCCC(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_outputs):\n",
    "        super(SCCC, self).__init__()\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_features, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, n_outputs),\n",
    "            #torch.nn.Softmax()\n",
    "        )\n",
    "            \n",
    "        initialize_weights(self.out)\n",
    "\n",
    "    def step(self, inputs, hidden=None, verbose=False):\n",
    "        #input = self.inp(input)\n",
    "        output = inputs #inputs.flatten(start_dim = 1, end_dim = 2)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, inputs, hidden=None, verbose=False):\n",
    "        batch_size = inputs.size(0)    \n",
    "        output, hidden = self.step(inputs, hidden, verbose=verbose)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constrnn = SimpleRNN(n_features=5 + (vision_bins * 8), n_outputs=4).cuda()\n",
    "sccc_model = SCCC(133, 1).cuda()\n",
    "#constrnn = torch.load('asdf.pt')\n",
    "#constrnn = torch.load('constcomplex1rnntrain.pt')\n",
    "\n",
    "#weight = torch.tensor([0.88675, 0.2187 , 3 , 3]).cuda()\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "#optimizer = torch.optim.SGD(sclass_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = nn.MSELoss()# Mean Squared Loss\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "l_rate = 0.001 \n",
    "#optimizer = torch.optim.SGD(hmodel.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(sccc_model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(sclass_model.parameters(), lr=0.005)\n",
    "#optimizer = torch.optim.RMSprop(sclass_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "import time\n",
    "import tables\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "data_from_runs = []\n",
    "\n",
    "\n",
    "eqltrain = np.asarray(eqltrain)\n",
    "h5ftest = tables.open_file('pathtofile.h5', mode='r')\n",
    "test = h5ftest.root.train[:, :, :]\n",
    "#print(\"lala test shape: \", test.shape)\n",
    "datarawsimplelast = []\n",
    "\n",
    "for samples_per_class in intervalls:\n",
    "    intervall_runs = []\n",
    "    for runs_per_intervall in range(0,runs_per_dataset):\n",
    "        print(runs_per_intervall + 1, \". run with \", samples_per_class, \"samples per class\")\n",
    "        batch_size = 32\n",
    "        train = []\n",
    "        train = eqltrain[np.random.choice(eqltrain.shape[0], samples_per_class*40), :, :]\n",
    "        sccc_model = SCCC(133, 1).cuda()\n",
    "        criterion = nn.MSELoss()# Mean Squared Loss\n",
    "        l_rate = 0.001 \n",
    "        optimizer = torch.optim.Adam(sccc_model.parameters(), lr=0.001)\n",
    "        train = np.array(train)\n",
    "        np.random.shuffle(train) #inplace op\n",
    "        print(train.shape)\n",
    "        print(\"samples per classes: \", train.shape[0]/40)\n",
    "        rec_losses = []\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "    \n",
    "        epochs = int(max(np.round(50000 / train.shape[0]), 10))\n",
    "        #TODO: MORE EPOCHS\n",
    "        #epochs = int(max(np.round(20000 / train.shape[0]), 10))\n",
    "        print(\"Epochs: \", epochs)\n",
    "        tqdm_epochs = tqdm_notebook(list(range(epochs)))\n",
    "        labels = []\n",
    "\n",
    "        tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "                                   total= 1)#train.shape[0] // batch_size, leave = False)\n",
    "        for epoch in range(0, epochs): #tqdm_epochs: \n",
    "            np.random.shuffle(train)\n",
    "            #tqdm_iter = tqdm.tqdm_notebook(enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)),\n",
    "            #                           total= train.shape[0] // batch_size, leave = False)#train.shape[0] // batch_size, leave = False)\n",
    "            for minibatch_idx, (batch_data) in enumerate(iterate_data(train , batchsize=batch_size, shuffle=False)):\n",
    "            #for minibatch_idx, (batch_data) in tqdm_iter: \n",
    "                i = i+1\n",
    "                batch_X = batch_data[:, -2, 6+4:].astype(np.float32) \n",
    "                #print(\"bx shape\", batch_X.shape)\n",
    "                labels = batch_data[:, -1, 9].astype(np.float32) \n",
    "                #print(labels)\n",
    "                #print(batch_data[:, -1, 6:12].astype(np.float32) )\n",
    "                batch_Y = labels\n",
    "                #print(\"by shape\", batch_Y.shape)\n",
    "                with autograd.detect_anomaly():\n",
    "                    sccc_model.zero_grad()\n",
    "                    batch_X = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "                    batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "                    outputs, hidden_input = sccc_model.forward(batch_X, verbose = False)\n",
    "                    loss = criterion(outputs.view(-1), batch_Y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "                    y_preds = np.concatenate((y_preds, y_pred))\n",
    "                    y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "                    y_trues = np.concatenate((y_trues, y_true))\n",
    "                    rec_losses.append(float(loss.cpu().data.numpy()))\n",
    "                    tqdm_epochs.set_description(\"Loss {:6.5f}\".format(np.mean(rec_losses[-100:])))   \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(16,4))\n",
    "        ax.plot(pd.Series(rec_losses).rolling(100).mean())\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "        batch_size = 2\n",
    "        y_preds = torch.empty((0, 1)).numpy()\n",
    "        y_trues = torch.empty((0, 1)).numpy()\n",
    "        label_counts = []\n",
    "\n",
    "        #clear_output()\n",
    "        for batch_data in iterate_data(test, batchsize=batch_size, shuffle=False):\n",
    "            batch_X = batch_data[:, -2, 6+4:].astype(np.float32)\n",
    "            labels = batch_data[:, -1, 9].astype(np.float32)\n",
    "            batch_Y = labels #group_wall_labels(labels)\n",
    "            batch_X = torch.autograd.Variable(torch.from_numpy(batch_X)).cuda()\n",
    "            batch_Y = torch.autograd.Variable(torch.from_numpy(batch_Y).float()).cuda()\n",
    "            outputs, hidden_input = sccc_model.forward(batch_X, verbose = False)\n",
    "            loss = criterion(outputs.view(-1), batch_Y)\n",
    "            y_pred = outputs.detach().cpu().numpy() #torch.argmax(outputs[:, :], 1).cpu().numpy()\n",
    "            y_preds = np.concatenate((y_preds, y_pred))\n",
    "            y_true = batch_Y.cpu().numpy()[:, None] #make_one_hot(batch_data[:, -1, 9].astype(np.float32), 40) \n",
    "            y_trues = np.concatenate((y_trues, y_true))\n",
    "        y_trues_oh = make_one_hot(np.round(y_trues), 40)\n",
    "        y_preds_oh = make_one_hot(np.round(y_preds), 40)\n",
    "    \n",
    "        # Split the data into a training set and a test set\n",
    "        y_test = np.argmax(y_trues_oh, axis = 1)\n",
    "        y_pred = np.argmax(np.round(y_preds_oh), axis = 1)\n",
    "        np.set_printoptions(precision=2)\n",
    "        # Plot non-normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None)\n",
    "        # Plot normalized confusion matrix\n",
    "        #plot_confusion_matrix(y_test, y_pred, classes=None, normalize=True)\n",
    "        #plt.show()\n",
    "    \n",
    "        errs = np.abs(y_test-y_pred)\n",
    "        plt.hist(errs)\n",
    "        plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        plt.show()\n",
    "        time.sleep(2)\n",
    "        print(\"error not bigger than 5: \", np.sum(errs < 5)/errs.shape[0])\n",
    "    \n",
    "        mean_err = np.sum(errs) / y_test.shape[0]\n",
    "        print(\"Durchschnittliche Abweichung: \", mean_err)\n",
    "    \n",
    "        standard_derivation_err = np.sqrt(1/(y_test.shape[0]-1) * np.sum(np.square(mean_err-errs)))\n",
    "        print(\"standard abweichung: \", standard_derivation_err)\n",
    "\n",
    "        cm = metrics.confusion_matrix(np.argmax(y_trues_oh, axis = 1), np.argmax(np.round(y_preds_oh), axis = 1), labels=None, sample_weight=None)\n",
    "        #print(cm.shape)    \n",
    "        print(\"richtig klassifiziert: \", cm.trace()/y_test.shape[0])\n",
    "        #data.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    \n",
    "        #plt.imshow(cm)\n",
    "        #plt.title('Samples per class: {}'.format(samples_per_class))\n",
    "        #plt.show()\n",
    "        \n",
    "        intervall_runs.append(np.array([samples_per_class, mean_err, standard_derivation_err, cm.trace()/y_test.shape[0]]))\n",
    "    datarawsimplelast.append(np.asarray(intervall_runs))\n",
    "    print(\"intervall_runs shape: \", len(intervall_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanprawsimplelast = np.mean(np.asarray(datarawsimplelast), axis = 1)\n",
    "datanprawsimplelast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_errorls = []\n",
    "std_correctls = []\n",
    "for intervall_data in datarawsimplelast:\n",
    "    print(intervall_data)\n",
    "    leninterveall = intervall_data.shape[0]\n",
    "    mean_error = np.mean(intervall_data[:, 1])\n",
    "    mean_correct = np.mean(intervall_data[:, 3])\n",
    "    std_errorls.append(np.sqrt(1/leninterveall * \n",
    "                            np.sum((intervall_data[:, 1] - mean_error)\n",
    "                                   *(intervall_data[:, 1] - mean_error))))\n",
    "    std_correctls.append(np.sqrt(1/leninterveall * \n",
    "                        np.sum((intervall_data[:, 3] - mean_correct)\n",
    "                                *(intervall_data[:, 3] - mean_correct))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawsimplelastline = plt.plot(datanprawsimplelast[:, 0], datanprawsimplelast[:, 1], 'co-', label=\"Last Step + Simple)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.plot(datanp[:, 0] * 40, datanp[:, 1], 'ro-', label=\"Hidden + Simple\")\n",
    "rawrnnline = plt.plot(datanprawrnn[:, 0] * 40, datanprawrnn[:, 1], 'bo-', label=\"Raw Data + RNN\")\n",
    "rawsimpleline = plt.plot(datanprawsimple[:, 0] * 40, datanprawsimple[:, 1], 'go-', label=\"Raw Data + Simple\")\n",
    "rawsimplelastline = plt.plot(datanprawsimplelast[:, 0] *40, datanprawsimplelast[:, 1], 'co-', label=\"Last Step + Simple\")\n",
    "\n",
    "plt.ylabel('Average error')\n",
    "plt.xlabel('Number of samples used for training')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend() #(loc=2, fontsize=\"small\")\n",
    "#plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.plot(datanp[:, 0] * 40, datanp[:, 1], 'ro-', label=\"Hidden + Simple\")\n",
    "rawrnnline = plt.plot(datanprawrnn[:, 0] * 40, datanprawrnn[:, 1], 'bo-', label=\"Raw Data + RNN\")\n",
    "rawsimpleline = plt.plot(datanprawsimple[:, 0] * 40, datanprawsimple[:, 1], 'go-', label=\"Raw Data + Simple\")\n",
    "rawsimplelastline = plt.plot(datanprawsimplelast[:, 0] *40, datanprawsimplelast[:, 1], 'co-', label=\"Last Step + Simple\")\n",
    "\n",
    "plt.ylabel('Average error')\n",
    "plt.xlabel('Number of samples used for training')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend() #(loc=2, fontsize=\"small\")\n",
    "#plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "hiddenrnnline = plt.plot(datanp[:, 0] * 40, datanp[:, 2], 'ro-', label=\"Hidden + Simple\")\n",
    "rawrnnline = plt.plot(datanprawrnn[:, 0] * 40, datanprawrnn[:, 2], 'bo-', label=\"Raw Data + RNN\")\n",
    "rawsimpleline = plt.plot(datanprawsimple[:, 0] * 40, datanprawsimple[:, 2], 'go-', label=\"Raw Data + Simple\")\n",
    "rawsimplelastline = plt.plot(datanprawsimplelast[:, 0] * 40, datanprawsimplelast[:, 2], 'co-', label=\"Last Step + Simple\")\n",
    "\n",
    "plt.ylabel('Average error')\n",
    "plt.xlabel('Number of samples used for training')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend() #(loc=2, fontsize=\"small\")\n",
    "#plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "hiddenrnnline = plt.plot(datanp[:, 0] * 40, datanp[:, 2], 'ro-', label=\"Hidden + Simple\")\n",
    "rawrnnline = plt.plot(datanprawrnn[:, 0] * 40, datanprawrnn[:, 2], 'bo-', label=\"Raw Data + RNN\")\n",
    "rawsimpleline = plt.plot(datanprawsimple[:, 0] * 40, datanprawsimple[:, 2], 'go-', label=\"Raw Data + Simple\")\n",
    "rawsimplelastline = plt.plot(datanprawsimplelast[:, 0] * 40, datanprawsimplelast[:, 2], 'co-', label=\"Last Step + Simple\")\n",
    "\n",
    "plt.ylabel('Average error')\n",
    "plt.xlabel('Number of samples used for training')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend() #(loc=2, fontsize=\"small\")\n",
    "#plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "hiddenrnnline = plt.errorbar(datanp[:, 0], datanp[:, 1], datanp[:, 2], linestyle='None', marker='^')\n",
    "rawrnnline = plt.errorbar(datanprawrnn[:, 0], datanprawrnn[:, 1], datanprawrnn[:, 2], linestyle='None', marker='^')\n",
    "rawsimpleline = plt.errorbar(datanprawsimple[:, 0], datanprawsimple[:, 1], datanprawsimple[:, 2], linestyle='None', marker='^')\n",
    "rawsimplelastline = plt.errorbar(datanprawsimplelast[:, 0], datanprawsimplelast[:, 1], datanprawsimple[:, 2], linestyle='None', marker='^')\n",
    "plt.xscale('log')\n",
    "plt.legend((hiddenrnnline, rawrnnline), ('Hidden + RNN', 'Raw Data + RNN'))\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(datanp[:, 0] * 40, datanp[:, 3]*100, 'ro-', label=\"Hidden + Simple\")\n",
    "plt.plot(datanprawrnn[:, 0] * 40, datanprawrnn[:, 3]*100, 'bo-', label=\"Raw Data + RNN\")\n",
    "plt.plot(datanprawsimple[:, 0] * 40, datanprawsimple[:, 3]*100, 'go-', label=\"Raw Data + Simple\")\n",
    "plt.plot(datanprawsimplelast[:, 0] * 40, datanprawsimplelast[:, 3]*100, 'co-', label=\"Last Step + Simple\")\n",
    "plt.xlabel('Number of samples used for training')\n",
    "plt.ylabel('Correctly classified in %')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
